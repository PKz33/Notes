## 推荐系统   
- **分类和回归**   
监督学习问题主要可以划分为两类，即分类问题和回归问题：分类问题预测数据属于哪一类别（离散）；回归问题根据数据预测一个数值（连续）   
- **分类问题**  
1. 在监督学习中，当输出变量Y取有限个离散值时，预测问题就成了分类问题   
2. 监督学习从数据中学习一个分类模型或分类决策函数，称为分类器；分类器对新的输入进行预测，称为分类   
3. 分类问题包括学习和分类两个过程。学习过程中，根据已知的训练数据集利用学习方法学习一个分类器；分类过程中，利用已习得的分类器对新的输入实例进行分类 
4. 分类问题可以用很多学习方法解决，比如K近邻、决策树、感知机、逻辑斯谛回归、支持向量机、朴素贝叶斯法、神经网络等  
- **精确率和召回率**  
1. 评价分类器性能的指标一般是分类准确率，定义为：分类器对测试集正确分类的样本数与总样本数之比   
2. 对于二分类问题，常用的评价指标是精确率与召回率   
3. 通常以关注的类为正类，其它为负类，按照分类器在测试集上预测的正确与否，会有四种情况出现，它们的总数分别记作：   
a. `TP`：将正类预测为正类的数目     
b. `FN`：将正类预测为负类的数目  
c. `FP`：将负类预测为正类的数目  
d. `TN`：将负类预测为负类的数目  
4. 精确率：`P = TP / (TP + FP)`，指的是所有预测为正类的数据中，预测正确的比例  
5. 召回率：`R = TP / (TP + FN)`，指的是所有实际为正类的数据中，预测正确的比例  
- **回归问题**  
1. 回归问题用于预测输入变量和输出变量之间的关系  
2. 回归模型就是表示从输入变量到输出变量之间映射的函数  
3. 回归问题的学习等价于函数拟合：选择一条函数曲线，使其很好地拟合已知数据，并且能够很好地预测未知数据  
4. 回归问题的分类：按照输入变量的个数：一元回归和多元回归；按照模型类型：线性回归和非线性回归  
5. 回归学习的损失函数：平方损失函数。如果选取平方损失函数作为损失函数，回归问题可以用著名的最小二乘法求解
- **模型求解算法（学习方法）**  
- **梯度下降算法**  
1. 梯度下降是一种常用的一阶优化方法，是求解无约束化问题最简单、最经典的方法之一  
2. 梯度方向：函数变化增长最快的方向（变量沿此方向变化时函数增长最快）  
3. 负梯度方向：函数变化减少最快的方向（变量沿此方向变化时函数减少最快）  
4. 损失函数是系数的函数，如果系数沿着损失函数的负梯度方向变化，此时损失函数减少最快，能够以最快速度下降到极小值  
5. 梯度下降不一定能够找到全局最优解，有可能是一个局部最优解。如果损失函数是下凸函数，梯度下降法得到的解就一定是全局最优解 
![](./Pics/梯度下降法.png)  
- **牛顿法和拟牛顿法**  
![](./Pics/牛顿法和拟牛顿法.png)
- **Python基础**  
```
  # Python可以使用引号、双引号、三引号表示字符串
  # 三引号可以由多行组成，常用于文档字符串，在文件的特定地点，被当做注释

  # 等待用户输入
  input()
  
  # Python可以在同一行中使用多条语句，语句之间使用分号分割
  
  if expression:
    suite
  elif expression:
    suite
  else:
    suite
    
  # 换行输出
  print("a")
  # 不换行输出
  print("a", end = "")
  
  # 将整个模块导入
  import somemodule
  # 从某个模块导入某个函数
  from somemodule import somefunction
  # 从某个模块导入多个函数
  from somemodule import firstfunc, secondfunc
  # 将某个模块中的全部函数导入
  from somemodule import *
  
  # 整型变量
  counter = 100
  # 浮点型变量
  miles = 1000.0
  # 字符串
  name = "pkz"
  
  # 多变量赋值
  a = b = c = 1
  a, b, c = 1, 2, "pkz"
  
  # 变量的交换
  a, b = b, a
  
  a, b = 10, 20
  a, b = b, a + 5
  print(a, b)
  # 20 15
  
  # 标准数据类型
  # Number（数字）、String（字符串）、List（列表）、Tuple（元组）、Set（集合）、Dictionary（字典）
  # 不可变：Number、String、Tuple
  # 可变：List、Dictionary、Set
  
  # Number
  # int、float、bool、complex
  a, b, c, d = 20, 5.5, True, 5 + 4j
  print(type(a), type(b), type(c), type(d))
  
  # 函数
  type(a) # 判断数据类型
  isinstance(a, int)  # 判断数据是否属于某类型
  del var1, var2  # 手动GC
  # type()不会认为子类是一种父类类型
  # isinstance()会认为子类是一种父类类型
  class A:
    pass
  class B(A):
    pass
  print(type(A()) == A) # True
  print(type(B()) == A) # False
  print(isinstance(A(), A)) # True
  print(isinstance(B(), A)) # True
  
  # 进制
  # 二进制：0b开头
  0b1010
  # 八进制：0o开头
  0o555
  # 十六进制：0x开头
  0x52A74 # 大小写都可以
  
  # python中没有数字的大小限制，可以使用任意大的数字
  # python可以保证整数运算的精确，但是浮点数运算时可能会得到一个不精确的结果
  
  # 数学函数
  import math
  # 基本数学函数
  abs(x)  # 返回x的绝对值
  ceil(x) # 返回x的上入整数
  (x > y) - (x < y) # 如果x<y返回-1，如果x==y返回0，如果x>y返回1
  exp(x)  # 返回e的x次幂
  fabs(x) # 返回x的绝对值
  floor(x)  # 返回x的下舍整数
  log(x)  # math.log(math.e)返回1.0，math.log(100,10)返回2.0
  log10(x)  # 返回以10为基数的x的对数
  max(x1, x2, ...)  # 返回最大值
  min(x1, x2, ...)  # 返回最小值
  modf(x) # 返回x的整数部分与小数部分，两部分的符号与x相同，整数部分以浮点型表示
  pow(x, y) # x**y运算后的值
  round(x [,n])  # 返回浮点数x的四舍五入值，如给出n值，则代表舍入到小数点后的位数
  sqrt(x) # 返回数字x的平方根
  
  # 随机数函数
  choice(seq) # 从序列的元素中随机挑选一个元素，比如random.choice(range(10))，从0到9中随机挑选一个整数
  randrange([start,] stop [,step])  # 从指定范围内，按指定基数递增从集合中获取一个随机数
  random()  # 随机生成一个范围在[0, 1)的实数
  seed([x]) # 改变随机数生成器的种子seed
  shuffle(lst)  # 将序列中所有元素随机排列
  uniform(x, y) # 随机生成一个范围在[x, y]的实数
  
  # 三角函数
  acos(x) # 返回x的反余弦弧度值
  asin(x) # 返回x的反正弦弧度值
  atan(x) # 返回x的反正切弧度值
  atan2(y, x) # 返回给定的x及y坐标值的反正切值
  cos(x) # 返回x的弧度的余弦值
  hypot(x, y) # 返回欧几里德范数sqrt(x*x+y*y)
  sin(x) # 返回x的正弦值
  tan(x) # 返回x的正切值
  degrees(x) # 将弧度转换为角度，如degrees(math.pi/2)，返回90.0
  radians(x) # 将角度转换为弧度
  
  # 数学常量
  pi # 圆周率
  e # 自然常数
  
  # String
  # Python中的字符串用单引号或双引号括起来，同时使用反斜杠\转义特殊字符
  # 加号+是字符串的连接符，星号*表示复制当前字符串，后面的数字为复制的次数
  
  # 字符串截取：变量[头下标 : 尾下标 : 步长]
  
  # 字符串打印
  # Python使用反斜杠\转义特殊字符，如果不想让反斜杠发生转义，可以在字符串前添加一个r，表示原始字符串
  print('pk\nz')
  print(r'pk\nz')
  
  # 字符串获取
  # Python中的字符串有两种索引方式，从左往右以0开始，从右往左以-1开始
  # Python中的字符串不能改变
  # Python中没有单独的字符类型，一个字符就是长度为1的字符串
  print(str[0])
  
  # List
  # 列表是写在方括号[]之间，用逗号分隔开的元素列表
  # 列表中元素的类型可以不相同，支持数字、字符串，也可以包含新列表
  # 列表可以被索引和截取，被截取后返回一个包含所需元素的新列表
  # 列表可以使用+操作符进行拼接
  # 列表中的元素是可以改变的
  # 列表不支持与或非运算
  list = [0, 1, 2, 'a']
  list[0] # 0
  len(list) # 4
  list[0 : 3] # [0, 1, 2]
  list[0] = 3
  
  # Tuple
  # 元组与列表类似，不同之处在于元组的元素不能修改
  # 元组写在小括号()里，元素之间用逗号隔开
  t = (0, 1, 2, 'a')
  t[0] # 0
  len(t) # 4
  t[0 : 3] # (0, 1, 2)
  # t[0] = 3 报错
  # 虽然元组的元素不可改变，但可以包含可变的对象，比如list
  list = [0, 1, 2]
  t = (0, 1, list)
  # t[0] = 1 报错
  t[2][0] = 1
  # 元组可以被索引和切片
  # 构造包含0或1个元素的元组的特殊语法规则
  tup = () # 空元组
  tup = (1,) # 一个元素的元组需要在元素后添加逗号
  # 元组可以使用+或*操作符进行拼接
  
  # Set
  # 集合基本功能是进行成员关系测试和删除重复元素
  # 可以使用大括号{}或set()函数创建集合
  # 创建一个空集合必须用set()而不是{}，因为{}是用来创建一个空字典
  s = {'a', 1, 2}
  s = set('1a1a2b2') # {'a', 'b', '1', '2'}
  sa = {'a', 'b', 'c'}
  sb = {'a', 'd'}
  print(sa - sb) # {'c', 'b'} sa和sb的差集
  print(sa | sb) # {'c', 'a', 'b', 'd'} sa和sb的并集
  print(sa & sb) # {'a'} sa和sb的交集
  print(sa ^ sb) # {'c', 'b', 'd'} sa和sb中不同时存在的元素
  
  # Dictionary
  # 列表是有序的对象集合，字典是无序的对象集合
  # 字典当中的元素是通过键来存取的
  # 字典用{}标识，同一个字典中，键(key)必须是唯一的
  d = {'a':1, 'b':2, 'c':3}
  d = dict([('a',3), ('b',2), ('c',1)])
  d = {x : x ** 2 for x in (2, 4, 6)}
  d = dict(a = 1, b = 3, c = 2)
  print(d['a'])
  print(d.keys()) # 输出所有键 dict_keys(['a', 'b', 'c'])
  print(d.values()) # 输出所有值 dict_values([1, 2, 3])
  
  # 函数
  # 基本语法
  def 函数名(参数列表):
    函数体
  
  # 在函数调用中输入参数时，参数名称必须对应
  # 当调用函数时，参数必须全部传入，且名称对应，顺序可以不同
  def aa(x, y):
    print(x)
  aa(x=2, y=3) # 2
  aa(2, 3) # 2
  aa(y=2, x=3) # 3
  
  # 函数中可以定义默认值
  # 有默认值的参数必须定义在无默认值的参数的后面
  def aa(x, y=1):
    print(y)
  aa(2) # 1
  aa(x=2) # 1
  aa(2, y=3) # 3
  # aa(y=2) 报错
  
  # 不定长度参数
  def aa(x, *args, **kwargs):
    print(x)
    print(args) # 元组
    print(kwargs) # 字典
  aa(1, 2, 3, 4, a=1, b=2)
  
  # 函数有返回值
  # 单个返回值
  def aa(x):
    return x
  a = aa(10)
  
  # 多个返回值
  def aa(x):
    return x, 10
  a = aa(10) # a是一个元组
  a, b = aa(10) # 多个参数接收
  
  # 匿名函数
  lambda [arg1 [,arg2, ...argn]]:expression
  sum = lambda arg1, arg2: arg1 + arg2
  sum(1, 2) # 3
```
- **线性回归模型**  
1. 线性回归是一种线性模型，它假设输入变量x和单个输出变量y之间存在线性关系。具体来说，利用线性回归模型，可以从一组输入变量x的线性组合中，计算输出变量y  
`y = ax + b`，`f(x) = w1x1 + w2x2 + ... + wdxd + b`  
![](./Pics/线性回归模型_1.png)   
2. 给定d个属性（特征）描述的示例`x = (x1; x2; ...; xd)`，其中`xi`是`x`在第`i`个属性（特征）上的取值，线性模型试图学得一个通过属性（特征）的线性组合来进行预测的函数，即`f(x) = w1x1 + w2x2 + ... + wdxd + b`，一般用向量形式写成`f(x) = wTx + b`，其中`w = (w1, w2, ..., wd)`  
3. 假设特征和结果都满足线性，即不大于一次方。`w`和`b`学得之后，模型就得以确定  
4. 许多功能更为强大的非线性模型可在线性模型的基础上通过引入层级结构或高维映射得到  
- **最小二乘法**  
![](./Pics/最小二乘法_1.png)
1. 基于均方误差最小化来进行模型求解的方法称为“最小二乘法”，它的主要思想是选择未知参数，使得理论值与观测值之差的平方和达到最小  
2. 假设输入属性（特征）的数目只有一个，`f(xi) = wxi + b`，使得`f(xi) ≈ yi`，在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小  
![](./Pics/最小二乘法_2.png)  
- **最小二乘法求解线性回归**  
![](./Pics/求解线性回归_1.png)
- **最小二乘算法实现**  
```
  # 0.引入依赖
  import numpy as np
  import matplotlib.pyplot as plt

  # 1.导入数据
  points = np.genfromtxt('./data/linear_data.csv', delimiter = ',')
  
  # 提取points中的两列数据，分别作为x，y
  x = points[:, 0]
  y = points[:, 1]
  
  # 2.定义损失函数
  # 损失函数是系数的函数，另外还需传入数据的x，y
  def compute_cost(w, b, points):
    total_cost = 0
    M = len(points)
    
    # 逐点计算平方损失误差，然后求平均数
    for i in range(M):
        x = points[i, 0]
        y = points[i, 1]
        total_cost += (y - w * x - b) ** 2
    
    return total_cost / M
    
  # 3.定义算法拟合函数
  # 先定义一个求均值的函数
  def average(data):
    sum = 0
    num = len(data)
    for i in range(num):
        sum += data[i]
    return sum / num

  # 定义核心拟合函数
  def fit(points):
    M = len(points)
    x_bar = average(points[:, 0])
    
    sum_yx = 0
    sum_x2 = 0
    sum_delta = 0
    
    for i in range(M):
        x = points[i, 0]
        y = points[i, 1]
        sum_yx += y * (x - x_bar)
        sum_x2 += x ** 2
    
    # 根据公式计算w
    w = sum_yx / (sum_x2 - M * (x_bar ** 2))
    
    for i in range(M):
        x = points[i, 0]
        y = points[i, 1]
        sum_delta += (y - w * x)
    b = sum_delta / M
    
    return w, b
    
  # 4.测试
  w, b = fit(points)

  print('w is：', w) # w is： 9449.962321455074
  print('b is：', b) # b is： 25792.20019866871

  cost = compute_cost(w, b, points)

  print('cost is：', cost) # cost is： 31270951.722280964
  
  # 5.画出拟合曲线
  plt.scatter(x, y)

  # 针对每一个x，计算出预测的y值
  pred_y = w * x + b

  plt.plot(x, pred_y, c = 'r')
  plt.show()
```  
![](./Pics/线性回归_1.png)  
- **多元线性回归**  
如果有两个或两个以上的自变量，这样的线性回归分析就称为多元线性回归。实际问题中，一个现象往往是受多个因素影响的，所以多元线性回归比一元线性回归的实际应用更广  
![](./Pics/多元线性回归_1.png)  
- **梯度下降法求解线性回归**  
![](./Pics/梯度下降法_1.png)
![](./Pics/梯度下降法_2.png)  
`α`在梯度下降算法中被称作学习率或者步长，可以通过`α`来控制每一步走的距离，保证不要走太快，错过了最低点；同时也要保证收敛速度不要太慢。所以`α`的选择在梯度下降法中往往是很重要的，不能太大也不能太小  
![](./Pics/梯度下降法_3.png)
- **梯度下降法和最小二乘法**  
1. 相同点：   
a. 本质和目标相同：两种方法都是经典的学习算法，在给定已知数据的前提下利用求导算出一个模型（函数），使得损失函数最小，然后对给定的新数据进行估算预测  
2. 不同点：  
a. 损失函数：梯度下降可以选取其它损失函数，最小二乘一定是平方损失函数  
b. 实现方法：最小二乘法是直接求导找出全局最小，梯度下降是一种迭代法  
c. 效果：最小二乘找到的是全局最小，但计算繁琐，且复杂情况下未必有解；梯度下降迭代计算简单，但找到的一般是局部最小，只有在目标函数是下凸函数时才是全局最小；梯度下降法到最小点附近时收敛速度会变慢，且对初始点的选择极为敏感  
- **梯度下降算法实现**  
```
  # 0. 引入依赖
  import numpy as np
  import matplotlib.pyplot as plt

  # 1. 导入数据
  points = np.genfromtxt('./data/linear_data.csv', delimiter = ',')

  # 提取points中的两列数据，分别作为x，y
  x = points[:, 0]
  y = points[:, 1]

  # 2. 定义损失函数
  # 损失函数是系数的函数，另外还要传入数据的x，y
  def compute_cost(w, b, points):
    total_cost = 0
    M = len(points)
    
    # 逐点计算平方损失误差，然后求平均数
    for i in range(M):
        x = points[i, 0]
        y = points[i, 1]
        total_cost += (y - w * x - b) ** 2
    
    return total_cost / M

  # 3. 定义模型的超参数
  alpha = 0.0001
  initial_w = 0
  initial_b = 0
  num_iter = 500

  # 4. 定义核心梯度下降算法函数
  def grad_desc(points, initial_w, initial_b, alpha, num_iter):
    w = initial_w
    b = initial_b
    # 定义一个list保存所有的损失函数值，用来显示下降的过程
    cost_list = []
    
    for i in range(num_iter):
        cost_list.append(compute_cost(w, b, points))
        w, b = step_grad_desc(w, b, alpha, points)
        
    return [w, b, cost_list]

  def step_grad_desc(current_w, current_b, alpha, points):
    sum_grad_w = 0
    sum_grad_b = 0
    M = len(points)
    
    # 对每个点，代入公式求和
    for i in range(M):
        x = points[i, 0]
        y = points[i, 1]
        sum_grad_w += (current_w * x + current_b - y) * x
        sum_grad_b += current_w * x + current_b - y
        
    # 用公式求当前梯度
    grad_w = 2 / M * sum_grad_w
    grad_b = 2 / M * sum_grad_b
    
    # 梯度下降，更新当前的w和b
    updated_w = current_w - alpha * grad_w
    updated_b = current_b - alpha * grad_b
    
    return updated_w, updated_b

  # 5. 测试：运行梯度下降算法计算最优的w和b
  w, b, cost_list = grad_desc(points, initial_w, initial_b, alpha, num_iter)

  print('w is：', w) # w is： 12594.124174942575
  print('b is：', b) # b is： 2379.4111288388417

  cost = compute_cost(w, b, points)
  print('cost is：', cost) # cost is： 153214370.681906

  plt.plot(cost_list)
  plt.show()
```  
![](./Pics/梯度下降实现.png)  
![](./Pics/梯度下降实现_1.png)
- **sklearn的LinearRegression**  
```
  # 0. 引入依赖
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.linear_model import LinearRegression

  # 1. 导入数据
  points = np.genfromtxt('./data/linear_data.csv', delimiter = ',')

  # 提取points中的两列数据，分别作为x，y
  x = points[:, 0]
  y = points[:, 1]

  # 2. 定义损失函数
  # 损失函数是系数的函数，另外还要传入数据的x，y
  def compute_cost(w, b, points):
    total_cost = 0
    M = len(points)
    
    # 逐点计算平方损失误差，然后求平均数
    for i in range(M):
        x = points[i, 0]
        y = points[i, 1]
        total_cost += (y - w * x - b) ** 2
    
    return total_cost / M

  lr = LinearRegression()

  x_new = x.reshape(-1, 1)
  y_new = y.reshape(-1, 1)

  lr.fit(x_new, y_new)

  # 从训练好的模型中提取系数和截距
  w = lr.coef_[0][0]
  b = lr.intercept_[0]

  print('w is：', w) # w is： 9449.962321455074
  print('b is：', b) # b is： 25792.20019866871

  cost = compute_cost(w, b, points)

  print('cost is：', cost) # cost is： 31270951.722280964

  plt.scatter(x, y)
  # 针对每一个x，计算出预测的y值
  pred_y = w * x + b

  plt.plot(x, pred_y, c = 'r')
  plt.show()
```
![](./Pics/Sklearn_LinearRegression.png)  
- **K近邻**  
![](./Pics/K近邻_1.png)  
`K近邻（k-nearest neighbour，KNN）`是一种基本分类方法，通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的k个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别，则该样本也属于这个类别，其中K通常是不大于20的整数。`KNN`算法中，所选择的邻居都是已经正确分类的对象  
![](./Pics/KNN示例.png)  
![](./Pics/KNN距离计算.png)  
- **KNN算法**  
在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个类，算法描述为：   
a. 计算测试数据与各个训练数据之间的距离  
b. 按照距离的递增关系进行排序  
c. 选取距离最小的K个点  
d. 确定前K个点所在类别的出现频率  
e. 返回前K个点中出现频率最高的类作为测试数据的预测分类  
- **KNN算法实现**  
```
# 0. 引入依赖
import numpy as np
import pandas as pd

# 直接引入sklearn里的数据集，iris
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split # 切分数据集为训练集和测试集
from sklearn.metrics import accuracy_score # 计算分类预测的准确率

# 1. 数据加载和预处理
iris = load_iris()
df = pd.DataFrame(data = iris.data, columns = iris.feature_names)
df['class'] = iris.target
df['class'] = df['class'].map({0 : iris.target_names[0], 1 : iris.target_names[1], 2 : iris.target_names[2]})
# df.head()
# df.describe()

x = iris.data
y = iris.target.reshape(-1, 1)
# print(x.shape, y.shape)

# 划分训练集和测试集
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 35, stratify = y)
# print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)

# x_test[0].reshape(1, -1).shape
# np.sum(np.abs(x_train - x_test[0].reshape(1, -1)), axis = 1)
# dist = np.array([3, 2, 532, 3243, 432])
# nn_index = np.argsort(dist)
# nn_y = y_train[nn_index[: 6]].ravel()
# np.bincount(nn_y)
# print(nn_y)
# print(np.bincount(nn_y))
# print(np.argmax(np.bincount(nn_y)))

# 2. 核心算法实现
# 距离函数定义
def l1_distance(a, b):
    return np.sum(np.abs(a - b), axis = 1)
def l2_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2, axis = 1))

# 分类器实现
class kNN(object):
    # 定义一个初始化方法，__init__是类的构造方法
    def __init__(self, n_neighbors = 1, dist_func = l1_distance):
        self.n_neighbors = n_neighbors
        self.dist_func = dist_func
        
    # 训练模型方法
    def fit(self, x, y):
        self.x_train = x
        self.y_train = y
        
    # 模型预测方法
    def predict(self, x):
        # 初始化预测分类数组
        y_pred = np.zeros((x.shape[0], 1), dtype = self.y_train.dtype)
        
        # 遍历输入的x数据点，取出每一个数据点的序号i和数据x_test
        for i, x_test in enumerate(x):
            # x_test跟所有训练数据计算距离
            distances = self.dist_func(self.x_train, x_test)
            
            # 得到的距离按照由近到远排序，取出索引值
            nn_index = np.argsort(distances)
            
            # 选取最近的k个点，保存它们对应的分类类别
            nn_y = self.y_train[nn_index[: self.n_neighbors]].ravel()
            
            # 统计类别中出现频率最高的那个，赋给y_pred[i]
            y_pred[i] = np.argmax(np.bincount(nn_y))
            
        return y_pred

# 3. 测试
# 定义一个knn实例
knn = kNN(n_neighbors = 3)
# 训练模型
knn.fit(x_train, y_train)

# 传入测试数据，做预测
y_pred = knn.predict(x_test)

# 求出预测准确率
accuracy = accuracy_score(y_test, y_pred)

# print('预测准确率：', accuracy) # 预测准确率： 0.9333333333333333

# 保存结果list
result_list = []

# 针对不同的参数选取，做预测
for p in [1, 2]:
    knn.dist_func = l1_distance if p == 1 else l2_distance
    
    # 考虑不同的k取值，步长为2
    for k in range(1, 10, 2):
        knn.n_neighbors = k
        # 传入测试数据，作预测
        y_pred = knn.predict(x_test)
        # 求出预测准确率
        accuracy = accuracy_score(y_test, y_pred)
        result_list.append([k, 'l1_distance' if p == 1 else 'l2_distance', accuracy])

df = pd.DataFrame(result_list, columns = ['k', '距离函数', '预测准确率'])
df
```
![](./Pics/KNN_iris数据集准确率.png)
- **逻辑斯蒂回归**  
![](./Pics/逻辑斯蒂回归_1.png)  
![](./Pics/逻辑斯蒂回归_2.png)  
![](./Pics/逻辑斯蒂回归_3.png)  
- **Sigmoid函数（压缩函数）**  
![](./Pics/Sigmoid函数_1.png)  
![](./Pics/Sigmoid函数_2.png)  
将线性回归拟合出来的值用压缩函数进行压缩，压缩完成后用`0.5`做一个概率的判定边界，就能把样本分成两类，即正样本和负样本  
![](./Pics/Sigmoid函数_3.png)  
`sigmoid`函数中，`e^(-z)`中`z`的正负决定`g(z)`的值最后是大于`0.5`还是小于`0.5`；即`z`大于`0`时，`g(z)`大于`0.5`，`z`小于`0`时，`g(z)`小于`0.5`  
当`z`对应的表达式为分类边界时，恰好有分类边界两侧对应`z`正负不同，也就使得分类边界两边分别对应`g(z)>0.5`和`g(z)<0.5`，因此根据`g(z)`与`0.5`的大小关系，就可以实现分类   
- **逻辑斯蒂回归的损失函数**    
![](./Pics/平方损失函数的问题.png)  
![](./Pics/逻辑斯蒂回归损失函数_1.png)  
![](./Pics/逻辑斯蒂回归损失函数_2.png)  
![](./Pics/逻辑斯蒂回归损失函数_3.png)  
![](./Pics/逻辑斯蒂回归损失函数_4.png)  
![](./Pics/逻辑斯蒂回归损失函数_5.png)  
![](./Pics/逻辑斯蒂回归损失函数_6.png)  
- **决策树**  
决策树是一种简单高效并且具有强解释性的模型，广泛应用于数据分析领域，其本质是一棵自上而下的由多个判断节点组成的树  
![](./Pics/决策树_1.png)  
- **决策树示例**  
预测小明今天是否会出门打球    
![](./Pics/决策树_2.png)  
![](./Pics/决策树_3.png)  
- **决策树与if-then规则**  
1. 决策树可以看作一个`if-then`规则的集合  
```
if(condition)  
  then
else
  then
```  
2. 由决策树的根节点到叶节点的每一条路径，构建一条规则：路径上内部节点的特征对应着规则的条件（`condition`），叶节点对应规则的结论  
3. 决策树的`if-then`规则集合有一个重要性质：互斥并且完备，即每个实例都被一条规则（一条路径）所覆盖，并且只被这一条规则覆盖  
4. `Condition`的确定过程就是特征选择的过程  
- **决策树的目标**  
1. 决策树学习的本质，是从训练数据中归纳出一组`if-then`分类规则  
2. 与训练集不相矛盾的决策树，可能有很多个，也可能一个也没有，所以需要选择一个与训练数据集矛盾较小的决策树  
3. 可以把决策树看成一个条件概率模型，目标是将实例分配到条件概率更大的那一类中去  
4. 从所有可能的情况中选择最优决策树，是一个NP完全问题，通常采用启发式算法求解决策树，得到一个次最优解  
5. 采用的算法通常是递归的进行以下过程：选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集都有一个最好的分类  
- **特征选择**    
特征选择就是决定用哪个特征来划分特征空间  
![](./Pics/决策树特征选择_1.png)  
- **随机变量**  
![](./Pics/决策树随机变量_1.png)    
1. 随机变量的本质是一个函数，是从样本空间的子集到实数的映射，将事件转换成一个数值  
2. 根据样本空间中的元素不同（即不同的实验结果），随机变量的值也将随机产生。可以说，随机变量是“数值化”的实验结果  
3. 在现实生活中，实验结果是描述性的词汇，比如“硬币的正面”、“反面”。在数学家眼里，这些文字化的叙述太过繁琐，所以拿数字代表它们  
- **熵**  
1. 熵用来衡量随机变量的不确定性。变量的不确定性越大，熵也就越大  
2. 设`X`是一个取有限个值得离散随机变量，其概率分布为：
![](./Pics/决策树熵_1.png)   
则随机变量`X`的熵定义为：  
![](./Pics/决策树熵_2.png)   
通常，上式中的对数以`2`为底或者以`e`为底（自然对数），这时熵的单位分别称为比特（`bit`）或纳特（`nat`）  
3. ![](./Pics/决策树熵_3.png)  
- **熵的示例**  
![](./Pics/决策树熵_4.png)    
![](./Pics/决策树熵_5.png)  
![](./Pics/决策树熵_6.png)  
![](./Pics/决策树熵_7.png)  
- **条件熵**  
![](./Pics/决策树条件熵_1.png)  
- **信息增益**  
![](./Pics/决策树信息增益_1.png)   
- **决策树的生成算法**  
1. `ID3`：决策树（`ID3`）的训练过程就是找到信息增益最大的特征，然后按照此特征进行分类，然后再找到各类型子集中信息增益最大的特征，然后按照此特征进行分类，最终得到符合要求的模型  
2. `C4.5`：`C4.5`算法在`ID3`基础上做了改进，用信息增益比来选择特征  
3. 分类与回归树（`CART`）：由特征选择、树的生成和剪枝三部分组成，既可以用于分类也可以用于回归  
- **K均值**  
![](./Pics/K均值_1.png)  
1. `k`均值（`k-means`）是据类算法中最为简单、高效的，属于无监督学习算法  
2. 核心思想：由用户指定`k`个初始质心（`initial centroids`），以作为聚类的类别（`cluster`），重复迭代直至算法收敛  
3. 基本算法流程：  
a. 选取`k`个初始质心（作为初始`cluster`）  
b. `repeat`：对每个样本点，计算得到距离其最近的质心，将其类别标为该质心所对应的`cluster`；重新计算`k`个`cluster`对应的质心  
c. 直到质心不再发生变化或迭代达到上限  
- **K均值算法实现**  
```
# 0. 引入依赖
import numpy as np
import matplotlib.pyplot as plt

# 从sklearn中直接生成聚类数据
from sklearn.datasets.samples_generator import make_blobs

# 引入scipy中的距离函数，默认欧式距离
from scipy.spatial.distance import cdist

# 1. 数据加载
x, y = make_blobs(n_samples = 100, centers = 6, random_state = 1234, cluster_std = 0.6)
# x.shape # (100, 2)

# plt.figure(figsize = (6, 6))
# plt.scatter(x[:, 0], x[:, 1], c = y)
# plt.show()

# 2. 算法实现
class K_Means(object):
    # 初始化，参数：聚类数n_clusters（K）、迭代次数max_iter、初始质心centroids
    def __init__(self, n_clusters = 6, max_iter = 300, centroids = []):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.centroids = np.array(centroids, dtype = np.float)
        
    # 训练模型方法，k-means聚类过程，传入原始数据
    def fit(self, data):
        # 假设没有指定初始质心，随机选取data中的点作为初始质心
        if(self.centroids.shape == (0,)):
            # 从data中随机生成0到data行数的6个整数，作为索引值
            self.centroids = data[np.random.randint(0, data.shape[0], self.n_clusters), :]
        
        # 开始迭代
        for i in range(self.max_iter):
            # 1. 计算距离矩阵，得到的是一个100*6的矩阵
            distances = cdist(data, self.centroids)
            
            # 2. 对距离按由近到远排序，选取最近的质心点的类别，作为当前的分类
            c_index = np.argmin(distances, axis = 1)
            
            # 3. 对每一类数据进行均值计算，更新质心点坐标
            for i in range(self.n_clusters):
                # 排除掉没有出现在c_index里的类别
                if i in c_index:
                    # 选出所有类别是i的点，取data里面坐标的均值，更新第i个质心
                    self.centroids[i] = np.mean(data[c_index == i], axis = 0)
                    
    # 实现预测方法
    def predict(self, samples):
        # 先计算距离矩阵，然后选取距离最近的那个质心的类别
        distances = cdist(samples, self.centroids)
        c_index = np.argmin(distances, axis = 1)
        
        return c_index
    
# 3. 测试
# 定义一个绘制子图函数
def plotKMeans(x, y, centroids, subplot, title):
    # 分配子图，121表示1行2列的子图中的第一个
    plt.subplot(subplot)
    plt.scatter(x[:, 0], x[:, 1], c = 'r')
    # 画出质心点
    plt.scatter(centroids[:, 0], centroids[:, 1], c = np.array(range(6)), s = 100)
    plt.title(title)
    
kmeans = K_Means(max_iter = 300, centroids = np.array([[3,1], [3,2], [3,3], [3,4], [3,5], [3,6]]))

plt.figure(figsize = (16, 6))
plotKMeans(x, y, kmeans.centroids, 121, 'Initial State')

# 开始聚类
kmeans.fit(x)

plotKMeans(x, y, kmeans.centroids, 122, 'Final State')

# 预测新数据点的类别
x_new = np.array([[0, 0], [12, 7], [-2.5, 6]])
y_pred = kmeans.predict(x_new)

# print(kmeans.centroids)
print(y_pred) # [1 5 4]

plt.scatter(x_new[:, 0], x_new[:, 1], s = 100, c = 'black')
    
# dist = np.array([[121, 221, 32, 43],
#                 [121, 1, 12, 23],
#                 [65, 21, 2, 43],
#                 [1, 221, 32, 43],
#                 [21, 11, 22, 3],])
# c_index = np.argmin(dist, axis = 1)
# print(c_index)
# x_new = x[0 : 5]
# print(x_new)
# print(c_index == 2)
# print(x_new[c_index == 2])
# np.mean(x_new[c_index == 2], axis = 0)
```  
![](./Pics/K均值_2.png)  
![](./Pics/K均值_3.png)  
![](./Pics/K均值_4.png)
- **基于人口统计学的推荐算法**  
![](./Pics/基于人口统计学的推荐算法.png)  
1. 基于人口统计学的推荐机制是一种最易于实现的推荐方法，它只是简单的根据系统用户的基本信息发现用户的相关程度，然后将相似用户喜爱的其他物品推荐给当前用户  
2. 对于没有明确含义的用户信息（比如登录时间、地域等上下文信息），可以通过聚类等手段，给用户打上分类标签  
3. 对于特定标签的用户，可以根据预设的规则或者模型，推荐出对应的物品  
- **用户画像**  
1. 用户信息标签化的过程一般称为用户画像  
2. 用户画像就是企业通过收集与分析消费者社会属性、生活习惯、消费行为等主要信息的数据，完美地抽象出一个用户的商业全貌  
3. 用户画像为企业提供了足够的信息基础，能够帮助企业快速找到精准用户群体以及用户需求等更为广泛的反馈信息  
4. 作为大数据的根基，用户画像完美地抽象出一个用户的信息全貌，为进一步精准、快速地分析用户行为习惯、消费习惯等重要信息，提供了足够的数据基础   
- **基于内容的推荐算法**  
![](./Pics/基于内容的推荐算法.png)   
1. 基于内容的推荐算法是根据推荐物品或内容的元数据，发现物品的相关性，再基于用户过去的喜好记录，为用户推荐相似的物品。通过抽取物品内在或外在的特征值，实现相似度计算（比如一个电影，有导演、演员、用户标签UGC、用户评论、时长、风格等等，都可以算是特征）  
2. 将用户个人信息特征（基于喜好记录或是预设兴趣标签），和物品的特征相匹配，就能得到用户对物品感兴趣的程度（有些网站还请专业的人员对物品进行基因编码/打标签（PGC））  
3. 对于物品的特征提取（打标签）：专家标签（`PGC`）、用户自定义标签（`UGC`）、降维分析数据，提取隐语义标签（`LFM`）  
4. 对于文本信息的特征提取（关键词）：分词、语义处理和情感分析（`NLP`）、潜在语义分析（`LSA`）  
- **相似度计算**  
![](./Pics/相似度计算.png)  
- **基于内容的推荐系统的高层次结构**  
![](./Pics/基于内容的推荐系统的高层次结构.png)  
- **特征工程**  
![](./Pics/特征工程_1.png)  
![](./Pics/特征工程_2.png)   
1. 特征工程是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程  
2. 特征：数据中抽取出来的对结果预测有用的信息，作为判断条件的一组输入变量，是做出判断的依据。特征的个数就是数据的观测维度。特征按照不同的数据类型分类（数值型、类别型、时间型、统计型），有不同的特征处理方法      
3. 目标：判断和预测的目标，模型的输出变量，是特征所产生的结果  
4. 特征工程一般包括特征清洗（采样、清洗异常样本），特征处理和特征选择  
- **数值型特征处理**  
1. 用连续数值表示当前维度特征，通常会对数值型特征进行数学上的处理，主要的做法是归一化和离散化  
2. 幅度调整/归一化：特征与特征之间应该是平等的，区别应该体现在特征内部   
![](./Pics/归一化公式.png)  
![](./Pics/归一化_1.png)  
![](./Pics/归一化_2.png)  
3. 离散化：将原始连续值切断，转化为离散值。让座问题：假设我们要训练一个模型判断在公交扯上应不应该给一个人让座，按照常理，应该是给年龄很大和年龄很小的人让座  
![](./Pics/离散化_1.png)  
![](./Pics/离散化_2.png)  
![](./Pics/离散化_3.png)  
